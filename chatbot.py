import os
from dotenv import load_dotenv
import streamlit as st
from typing import Literal
from dataclasses import dataclass
# LangChainæ ¸å¿ƒä¾èµ–
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder


#åºŸå¼ƒï¼Œç›®å‰openaiæ²¡æœ‰è°ƒé€š
# ä¿®å¤HTTPå¤´éƒ¨ç¼–ç é—®é¢˜ï¼ˆå…¼å®¹ä¸­æ–‡ä¸”é¿å…ç±»å‹å†²çªï¼‰
import httpx
from httpx._models import _normalize_header_value

def patch_normalize_header_value(value, encoding=None):
    """ä¿®æ­£å¤´éƒ¨å€¼ç¼–ç ï¼Œä¼˜å…ˆä¿ç•™å­—ç¬¦ä¸²ç±»å‹ï¼Œé¿å…byteså†²çª"""
    if isinstance(value, bytes):
        return value  # å·²ä¸ºbytesç±»å‹ç›´æ¥è¿”å›
    if not isinstance(value, str):
        raise TypeError(f"Header value must be str or bytes, not {type(value).__name__}")
    # ä¼˜å…ˆç”¨ASCIIç¼–ç ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢ä¸ºUTF-8ï¼ˆè§£å†³ä¸­æ–‡é—®é¢˜ï¼‰
    try:
        return value.encode(encoding or "ascii")
    except UnicodeEncodeError:
        return value.encode("utf-8")

# åº”ç”¨è¡¥ä¸
httpx._models._normalize_header_value = patch_normalize_header_value

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä».envæ–‡ä»¶ï¼‰
load_dotenv()

@dataclass
class Message:
    """èŠå¤©æ¶ˆæ¯æ•°æ®ç»“æ„"""
    origin: Literal["human", "ai"]  # æ¶ˆæ¯æ¥æºï¼šç”¨æˆ·/AI
    message: str  # æ¶ˆæ¯å†…å®¹

def build_llm(selected_model):
    """
    æ ¹æ®é€‰æ‹©çš„æ¨¡å‹æ„å»ºLLMå®ä¾‹
    è¿”å›ï¼š(llmå®ä¾‹, é”™è¯¯ä¿¡æ¯)
    """
    api_key = None
    base_url = None
    custom_headers = None  # è‡ªå®šä¹‰HTTPå¤´éƒ¨ï¼ˆé€‚é…DeepSeekï¼‰
    
    if selected_model == "OpenAI":
        # OpenAIé…ç½®
        api_key = str(os.getenv("OPENAI_API_KEY", "").strip())  # å¼ºåˆ¶å­—ç¬¦ä¸²ç±»å‹
        base_url = str(os.getenv("OPENAI_BASE_URL", "").strip() or None)
        default_model = "gpt-3.5-turbo"
    elif selected_model == "DeepSeek":
        # DeepSeeké…ç½®ï¼ˆæ˜¾å¼å¤„ç†å¤´éƒ¨é¿å…ç±»å‹é”™è¯¯ï¼‰
        api_key = str(os.getenv("DEEPSEEK_API_KEY", "").strip())
        base_url = str(os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1").strip())
        default_model = "deepseek-chat"
        # æ‰‹åŠ¨æ„å»ºAuthorizationå¤´éƒ¨ï¼ˆè§£å†³DeepSeekçš„APIå¯†é’¥ç±»å‹é—®é¢˜ï¼‰
        if api_key:
            custom_headers = {"Authorization": f"Bearer {api_key}"}
    
    # æ¨¡å‹åç§°ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > é»˜è®¤å€¼
    model = os.getenv(f"{selected_model.upper()}_MODEL") or default_model
    
    # æ ¡éªŒAPIå¯†é’¥
    if not api_key:
        return None, f"æœªæ£€æµ‹åˆ°{selected_model}çš„APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶"
    
    try:
        # åˆå§‹åŒ–LLMï¼ˆDeepSeeké€šè¿‡headersä¼ é€’å¯†é’¥ï¼Œé¿å…api_keyå‚æ•°ç±»å‹å†²çªï¼‰
        llm = ChatOpenAI(
            model=model,
            temperature=0.2,  # æ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼ˆ0.2è¾ƒç¨³å®šï¼‰
            base_url=base_url,
            api_key=api_key if selected_model != "DeepSeek" else None,  # DeepSeekç”¨headers
            default_headers=custom_headers  # ä¼ é€’è‡ªå®šä¹‰å¤´éƒ¨
        )
        return llm, None
    except Exception as e:
        return None, f"{selected_model}åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}"

def reset_session():
    """é‡ç½®ä¼šè¯çŠ¶æ€ï¼ˆæ¨¡å‹åˆ‡æ¢æ—¶è°ƒç”¨ï¼‰"""
    for key in ["token_count", "history", "conversation", "current_model"]:
        if key in st.session_state:
            del st.session_state[key]

def initialize_session_state(llm, selected_model):
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ï¼Œç¡®ä¿å˜é‡æ­£ç¡®èµ‹å€¼"""
    # æ¨¡å‹åˆ‡æ¢æ—¶å¼ºåˆ¶é‡ç½®ä¼šè¯
    if "current_model" in st.session_state and st.session_state.current_model != selected_model:
        reset_session()
    
    st.session_state.current_model = selected_model  # è®°å½•å½“å‰æ¨¡å‹
    
    # åˆå§‹åŒ–åŸºç¡€çŠ¶æ€ï¼ˆæ— ä¾èµ–é¡¹ï¼‰
    if "token_count" not in st.session_state:
        st.session_state.token_count = 0  # ç®€å•tokenè®¡æ•°
    if "history" not in st.session_state:
        st.session_state.history = []  # èŠå¤©å†å²
    
    # ä»…å½“LLMæœ‰æ•ˆæ—¶ï¼Œåˆå§‹åŒ–å¯¹è¯é“¾
    if llm is not None and "conversation" not in st.session_state:
        # 1. åˆ›å»ºè®°å¿†ç»„ä»¶ï¼ˆæ›¿ä»£è¿‡æ—¶çš„ConversationSummaryMemoryï¼‰
        conversation_memory = ConversationSummaryBufferMemory(
            llm=llm,
            memory_key="history",  # ä¸promptä¸­çš„å˜é‡åå¯¹åº”
            return_messages=True,  # è¿”å›Messageå¯¹è±¡è€Œéå­—ç¬¦ä¸²
            max_token_limit=1000  # æ§åˆ¶è®°å¿†é•¿åº¦ï¼ˆé¿å…è¶…é™ï¼‰
        )
        
        # 2. å®šä¹‰å¯¹è¯æç¤ºæ¨¡æ¿ï¼ˆç³»ç»Ÿè§’è‰²+å†å²+ç”¨æˆ·è¾“å…¥ï¼‰
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯åµŒå…¥åœ¨å¥èº«åº”ç”¨ä¸­çš„AIæ•™ç»ƒï¼Œæ“…é•¿æŒ‡å¯¼å®¶åº­é”»ç‚¼åŠ¨ä½œï¼Œèƒ½åˆ†ç±»å’Œè®¡æ•°é‡å¤åŠ¨ä½œï¼Œç”¨ä¸“ä¸šä¸”æ˜“æ‡‚çš„è¯­è¨€æä¾›å¥èº«å»ºè®®ã€‚"),
            MessagesPlaceholder(variable_name="history"),  # å¯¹è¯å†å²
            ("human", "{input}")  # ç”¨æˆ·è¾“å…¥
        ])
        
        # 3. æ„å»ºå¸¦å†å²çš„å¯¹è¯é“¾ï¼ˆæ›¿ä»£deprecatedçš„ConversationChainï¼‰
        chain = prompt | llm  # LCELè¯­æ³•ï¼šprompt -> llm
        st.session_state.conversation = RunnableWithMessageHistory(
            chain,
            lambda session_id: conversation_memory,  # è®°å¿†ç®¡ç†å‡½æ•°
            input_messages_key="input",  # ç”¨æˆ·è¾“å…¥çš„key
            history_messages_key="history"  # å†å²æ¶ˆæ¯çš„key
        )
    # è‹¥LLMæ— æ•ˆï¼Œæ¸…é™¤å¯¹è¯é“¾
    elif llm is None and "conversation" in st.session_state:
        del st.session_state.conversation

def on_click_callback():
    """æ¶ˆæ¯æäº¤å›è°ƒå‡½æ•°ï¼ˆå¸¦å®Œæ•´é”™è¯¯å¤„ç†ï¼‰"""
    human_prompt = st.session_state.get('human_prompt', '').strip()
    
    # åŸºç¡€æ ¡éªŒ
    if not human_prompt:
        st.warning("è¯·è¾“å…¥å†…å®¹åå†å‘é€")
        return
    if "conversation" not in st.session_state or st.session_state.conversation is None:
        st.error("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æˆ–é‡æ–°é€‰æ‹©æ¨¡å‹")
        return
    
    try:
        # è°ƒç”¨AIç”Ÿæˆå›å¤ï¼ˆæ˜¾ç¤ºåŠ è½½çŠ¶æ€ï¼‰
        with st.spinner("AIæ­£åœ¨æ€è€ƒ..."):
            # è°ƒç”¨å¯¹è¯é“¾ï¼ˆéœ€æŒ‡å®šsession_idåŒºåˆ†ä¼šè¯ï¼‰
            response = st.session_state.conversation.invoke(
                {"input": human_prompt},
                config={"configurable": {"session_id": "fitness_chat"}}  # å›ºå®šsession_id
            )
            llm_response = response.content  # æå–å›å¤å†…å®¹
        
        # æ›´æ–°èŠå¤©å†å²å’Œtokenè®¡æ•°
        st.session_state.history.append(Message("human", human_prompt))
        st.session_state.history.append(Message("ai", llm_response))
        st.session_state.token_count += len(human_prompt.split()) + len(llm_response.split())
    
    except Exception as e:
        st.error(f"å¯¹è¯å‡ºé”™ï¼š{str(e)}")
    
    # æ¸…ç©ºè¾“å…¥æ¡†
    st.session_state.human_prompt = ""

def chat_ui(context: str = ""):
    """æ¸²æŸ“èŠå¤©ç•Œé¢"""
    st.subheader("ğŸ’¬ AI å¥èº«æ•™ç»ƒ")
    
    # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
    selected_model = st.selectbox(
        "é€‰æ‹©å¤§æ¨¡å‹",
        ["OpenAI", "DeepSeek"],
        index=0,
        help="åˆ‡æ¢ä¸åŒçš„AIæ¨¡å‹è¿›è¡Œå¯¹è¯ï¼ˆéœ€é…ç½®å¯¹åº”APIå¯†é’¥ï¼‰"
    )
    
    # æ„å»ºæ¨¡å‹å¹¶åˆå§‹åŒ–ä¼šè¯
    llm, err = build_llm(selected_model)
    initialize_session_state(llm, selected_model)
    
    # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼ˆå¦‚APIå¯†é’¥ç¼ºå¤±ï¼‰
    if err or llm is None:
        st.info(err or "èŠå¤©åŠŸèƒ½æœªå¯ç”¨")
        return

    # è‡ªå®šä¹‰èŠå¤©æ°”æ³¡æ ·å¼
    st.markdown("""
        <style>
            .chat-container { display: flex; flex-direction: column; gap: 10px; margin-bottom: 20px; }
            .chat-row { display: flex; width: 100%; }
            .row-reverse { flex-direction: row-reverse; }
            .chat-bubble { padding: 10px 15px; border-radius: 20px; max-width: 70%; word-wrap: break-word; }
            .user-bubble { background-color: #007bff; color: white; }
            .ai-bubble { background-color: #f0f0f0; color: black; }
        </style>
    """, unsafe_allow_html=True)
    
    # æ˜¾ç¤ºèŠå¤©å†å²
    with st.container():
        st.markdown('<div class="chat-container">', unsafe_allow_html=True)
        for chat in st.session_state.history:
            bubble_class = "user-bubble" if chat.origin == "human" else "ai-bubble"
            row_class = "chat-row row-reverse" if chat.origin == "human" else "chat-row"
            st.markdown(f'''
                <div class="{row_class}">
                    <div class="chat-bubble {bubble_class}">{chat.message}</div>
                </div>
            ''', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

    # ç”¨æˆ·è¾“å…¥è¡¨å•
    with st.form("chat-form", clear_on_submit=True):
        user_input = st.text_input(
            "è¯·è¾“å…¥ä½ çš„å¥èº«é—®é¢˜æˆ–éœ€æ±‚...", 
            key="human_prompt",
            placeholder="ä¾‹å¦‚ï¼šå¦‚ä½•æ­£ç¡®åšä¿¯å§æ’‘ï¼Ÿ"
        )
        submit_btn = st.form_submit_button("å‘é€", on_click=on_click_callback)

    # æ˜¾ç¤ºtokenè®¡æ•°ï¼ˆç®€å•ä¼°ç®—ï¼‰
    st.caption(f"ç´¯è®¡å¤§çº¦ä½¿ç”¨ {st.session_state.token_count} ä¸ªtoken")

# ä¸»å‡½æ•°å…¥å£
if __name__ == "__main__":
    chat_ui()