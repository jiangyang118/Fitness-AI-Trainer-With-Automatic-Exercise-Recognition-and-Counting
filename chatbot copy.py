import os
from dotenv import load_dotenv
import streamlit as st
from typing import Literal
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationSummaryMemory
from dataclasses import dataclass
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
#åºŸå¼ƒï¼Œç›®å‰openaiæ²¡æœ‰è°ƒé€š
@dataclass
class Message:
    origin: Literal["human", "ai"]
    message: str

def build_llm():
    """æ„å»ºLLMæ¨¡å‹å®ä¾‹å¹¶å¤„ç†åˆå§‹åŒ–é”™è¯¯"""
    base_url = os.getenv("OPENAI_BASE_URL", "").strip() or None
    api_key  = os.getenv("OPENAI_API_KEY", "").strip() or None
    # ç»Ÿä¸€ç”¨OPENAI_MODELï¼›è‹¥æ²¡é…ï¼ŒæŒ‰æ˜¯å¦æ¥DeepSeeké€‰æ‹©é»˜è®¤å€¼
    model = os.getenv("OPENAI_MODEL") or ("deepseek-chat" if base_url else "gpt-3.5-turbo")
    
    if not api_key:
        return None, "æœªæ£€æµ‹åˆ°OPENAI_API_KEYï¼ŒèŠå¤©å·²ç¦ç”¨ã€‚"
    
    try:
        llm = ChatOpenAI(
            model=model, 
            temperature=0.2,
            base_url=base_url, 
            api_key=api_key
        )
        return llm, None
    except Exception as e:
        return None, f"LLMåˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}"

def initialize_session_state(llm):
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ï¼Œç¡®ä¿åªåˆå§‹åŒ–ä¸€æ¬¡"""
    if "token_count" not in st.session_state:
        st.session_state.token_count = 0
    if "history" not in st.session_state:
        st.session_state.history = []
    if "conversation" not in st.session_state and llm:
        # åˆå§‹åŒ–å¯¹è¯è®°å¿†
        conversation_memory = ConversationSummaryMemory(
            llm=llm,
            memory_key="history"
        )
        # è®¾ç½®AIè§’è‰²ï¼šå¥èº«æ•™ç»ƒ
        conversation_memory.save_context(
            {"human": "ç³»ç»Ÿæç¤º"}, 
            {"ai": "ä½ æ˜¯åµŒå…¥åœ¨å¥èº«åº”ç”¨ä¸­çš„AIæ•™ç»ƒï¼Œæ“…é•¿æŒ‡å¯¼å®¶åº­é”»ç‚¼åŠ¨ä½œï¼Œèƒ½åˆ†ç±»å’Œè®¡æ•°é‡å¤åŠ¨ä½œï¼Œç”¨ä¸“ä¸šä¸”æ˜“æ‡‚çš„è¯­è¨€æä¾›å¥èº«å»ºè®®ã€‚"}
        )
        st.session_state.conversation = ConversationChain(
            llm=llm,
            memory=conversation_memory,
        )

def on_click_callback():
    """å¤„ç†æ¶ˆæ¯æäº¤é€»è¾‘"""
    human_prompt = st.session_state.get('human_prompt', '').strip()
    if human_prompt and "conversation" in st.session_state:
        # è·å–AIå“åº”
        llm_response = st.session_state.conversation.run(human_prompt)
        # æ›´æ–°å¯¹è¯å†å²
        st.session_state.history.append(Message("human", human_prompt))
        st.session_state.history.append(Message("ai", llm_response))
        # ç®€å•çš„tokenè®¡æ•°ï¼ˆå®é™…åº”ç”¨éœ€ç”¨tiktokenï¼‰
        st.session_state.token_count += len(human_prompt.split()) + len(llm_response.split())
        # æ¸…ç©ºè¾“å…¥æ¡†
        st.session_state.human_prompt = ""

def chat_ui(context: str = ""):
    """èŠå¤©ç•Œé¢æ¸²æŸ“"""
    # æ„å»ºLLM
    llm, err = build_llm()
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    initialize_session_state(llm)
    
    st.subheader("ğŸ’¬ AI å¥èº«æ•™ç»ƒ")
    if err or llm is None:
        st.info(err or "èŠå¤©åŠŸèƒ½æœªå¯ç”¨")
        return

    # æ˜¾ç¤ºè‡ªå®šä¹‰CSS
    st.markdown("""
        <style>
            .chat-container {
                display: flex;
                flex-direction: column;
                gap: 10px;
            }
            .chat-row {
                display: flex;
                width: 100%;
            }
            .row-reverse {
                flex-direction: row-reverse;
            }
            .chat-bubble {
                padding: 10px 15px;
                border-radius: 20px;
                margin-bottom: 5px;
                max-width: 70%;
                word-wrap: break-word;
            }
            .user-bubble {
                background-color: #007bff;
                color: white;
            }
            .ai-bubble {
                background-color: #f0f0f0;
                color: black;
            }
        </style>
    """, unsafe_allow_html=True)

    # æ˜¾ç¤ºèŠå¤©å†å²
    chat_container = st.container()
    with chat_container:
        st.markdown('<div class="chat-container">', unsafe_allow_html=True)
        for chat in st.session_state.history:
            bubble_class = "user-bubble" if chat.origin == "human" else "ai-bubble"
            row_class = "chat-row row-reverse" if chat.origin == "human" else "chat-row"
            st.markdown(f'''
                <div class="{row_class}">
                    <div class="chat-bubble {bubble_class}">
                        {chat.message}
                    </div>
                </div>
            ''', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

    # è¾“å…¥è¡¨å•
    with st.form("chat-form", clear_on_submit=True):
        user_input = st.text_input("è¯·è¾“å…¥ä½ çš„å¥èº«é—®é¢˜æˆ–éœ€æ±‚...", 
                                 key="human_prompt",
                                 placeholder="ä¾‹å¦‚ï¼šå¦‚ä½•æ­£ç¡®åšä¿¯å§æ’‘ï¼Ÿ")
        submit_btn = st.form_submit_button("å‘é€", on_click=on_click_callback)

    # æ˜¾ç¤ºtokenè®¡æ•°ï¼ˆç®€å•ä¼°ç®—ï¼‰
    st.caption(f"ç´¯è®¡å¤§çº¦ä½¿ç”¨ {st.session_state.token_count} ä¸ªtoken")

if __name__ == "__main__":
    chat_ui()
